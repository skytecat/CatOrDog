{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FleR7ksOeTI_",
        "Nev2TupehdIH",
        "JGbwCsm4idft"
      ],
      "authorship_tag": "ABX9TyMX52Y9nxy7ASN92Etp3++i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skytecat/CatOrDog/blob/main/cat_and_dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорты"
      ],
      "metadata": {
        "id": "FleR7ksOeTI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import kagglehub\n",
        "!pip install tensorflow -q\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', module='PIL.TiffImagePlugin')"
      ],
      "metadata": {
        "id": "uhFaQeY8eSaM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Путь к датасету\n",
        "path = kagglehub.dataset_download(\"bhavikjikadara/dog-and-cat-classification-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_HHBWhAO6dj",
        "outputId": "70e44c80-c9dc-447d-e952-a3abf1439a06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/dog-and-cat-classification-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_folder = os.path.join(path, 'PetImages', 'Cat')\n",
        "dog_folder = os.path.join(path, 'PetImages', 'Dog')"
      ],
      "metadata": {
        "id": "Rnkgsd6v1Nce"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предварительный анализ датасета"
      ],
      "metadata": {
        "id": "Nev2TupehdIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_images_shapes(folder):\n",
        "    \"\"\"Проверка форматов изображений с обработкой ошибок\"\"\"\n",
        "    channels = {}\n",
        "    sizes = []\n",
        "    errors = 0\n",
        "    processed = 0\n",
        "\n",
        "    print(f\"Загрузка из папки: {os.path.basename(folder)}\")\n",
        "\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "            try:\n",
        "                img_path = os.path.join(folder, filename)\n",
        "\n",
        "                # Открываем изображение\n",
        "                img = Image.open(img_path)\n",
        "\n",
        "                # Проверяем, можно ли прочитать полностью\n",
        "                img.verify()  # Проверяет целостность\n",
        "\n",
        "                # Если verify прошел, открываем заново (он закрывает файл)\n",
        "                img = Image.open(img_path)\n",
        "                original_width, original_height = img.size\n",
        "                sizes.append((original_width, original_height))\n",
        "\n",
        "                img_array = np.array(img)\n",
        "\n",
        "                # Считаем статистику\n",
        "                shape = img_array.shape\n",
        "                if len(shape) == 2:\n",
        "                  channels[1] = channels.get(1, 0) + 1\n",
        "                else:\n",
        "                  channels[shape[2]] = channels.get(shape[2], 0) + 1\n",
        "                processed += 1\n",
        "\n",
        "                # Показываем прогресс каждые 1000 файлов\n",
        "                if processed % 1000 == 0:\n",
        "                    print(f\"  Обработано: {processed} файлов...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                errors += 1\n",
        "                if errors <= 10:\n",
        "                  print(f\"  Ошибка с {filename}: {type(e).__name__}\")\n",
        "\n",
        "    print(f\"\\n  Всего обработано: {processed} файлов\")\n",
        "    print(f\"  Ошибок: {errors}\")\n",
        "\n",
        "    return channels, sizes, processed, errors\n",
        "\n",
        "def analyze_sizes(sizes, class_name):\n",
        "    \"\"\"Анализ размеров изображений\"\"\"\n",
        "    if not sizes:\n",
        "        print(f\"Нет данных для анализа {class_name}\")\n",
        "        return\n",
        "\n",
        "    widths = [s[0] for s in sizes]\n",
        "    heights = [s[1] for s in sizes]\n",
        "    aspect_ratios = [w/h for w, h in sizes]\n",
        "\n",
        "    print(f\"\\nАНАЛИЗ РАЗМЕРОВ ИЗОБРАЖЕНИЙ {class_name.upper()}:\")\n",
        "    print(f\"  Ширина: от {min(widths)} до {max(widths)} (средняя: {np.mean(widths):.0f})\")\n",
        "    print(f\"  Высота: от {min(heights)} до {max(heights)} (средняя: {np.mean(heights):.0f})\")\n",
        "    print(f\"  Соотношение сторон: от {min(aspect_ratios):.2f} до {max(aspect_ratios):.2f}\")\n",
        "\n",
        "    print(f\"\\n  Классификация по размерам:\")\n",
        "    print(f\"    Очень маленькие (<32px): {sum(1 for w,h in sizes if w<32 or h<32)}\")\n",
        "    print(f\"    Маленькие (32-64px): {sum(1 for w,h in sizes if (w>=32 and w<64) or (h>=32 and h<64))}\")\n",
        "    print(f\"    Средние (64-128px): {sum(1 for w,h in sizes if (w>=64 and w<128) or (h>=64 and h<128))}\")\n",
        "    print(f\"    Большие (128-256px): {sum(1 for w,h in sizes if (w>=128 and w<256) or (h>=128 and h<256))}\")\n",
        "    print(f\"    Очень большие (256-512px): {sum(1 for w,h in sizes if (w>=256 and w<512) or (h>=256 and h<512))}\")\n",
        "    print(f\"    Огромные (>512px): {sum(1 for w,h in sizes if w>512 or h>512)}\")\n",
        "\n",
        "    print(f\"\\n  Классификация по соотношениям сторон:\")\n",
        "    print(f\"    Очень узкие (<0.5): {sum(1 for r in aspect_ratios if r < 0.5)}\")\n",
        "    print(f\"    Узкие (0.5-0.8): {sum(1 for r in aspect_ratios if r >= 0.5 and r < 0.8)}\")\n",
        "    print(f\"    Квадратные (0.8-1.2): {sum(1 for w,h in sizes if w==h or (min(w,h)/max(w,h) >= 0.8 and min(w,h)/max(w,h) <= 1.2))}\")\n",
        "    print(f\"    Широкие (1.2-2.0): {sum(1 for r in aspect_ratios if r >= 1.2 and r < 2.0)}\")\n",
        "    print(f\"    Очень широкие (>2.0): {sum(1 for r in aspect_ratios if r >= 2.0)}\")\n",
        "\n",
        "cat_channels, cat_sizes, cat_processed, cat_errors = check_images_shapes(\n",
        "    '/kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat'\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "dog_channels, dog_sizes, dog_processed, dog_errors = check_images_shapes(\n",
        "    '/kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idy2ZdQT-j3J",
        "outputId": "dacf65f5-113e-4be5-8f46-a54f5c22330a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загрузка из папки: Cat\n",
            "  Обработано: 1000 файлов...\n",
            "  Обработано: 2000 файлов...\n",
            "  Обработано: 3000 файлов...\n",
            "  Обработано: 4000 файлов...\n",
            "  Обработано: 5000 файлов...\n",
            "  Обработано: 6000 файлов...\n",
            "  Обработано: 7000 файлов...\n",
            "  Обработано: 8000 файлов...\n",
            "  Обработано: 9000 файлов...\n",
            "  Обработано: 10000 файлов...\n",
            "  Обработано: 11000 файлов...\n",
            "  Обработано: 12000 файлов...\n",
            "\n",
            "  Всего обработано: 12499 файлов\n",
            "  Ошибок: 0\n",
            "\n",
            "==================================================\n",
            "Загрузка из папки: Dog\n",
            "  Обработано: 1000 файлов...\n",
            "  Обработано: 2000 файлов...\n",
            "  Обработано: 3000 файлов...\n",
            "  Обработано: 4000 файлов...\n",
            "  Обработано: 5000 файлов...\n",
            "  Обработано: 6000 файлов...\n",
            "  Обработано: 7000 файлов...\n",
            "  Обработано: 8000 файлов...\n",
            "  Обработано: 9000 файлов...\n",
            "  Обработано: 10000 файлов...\n",
            "  Обработано: 11000 файлов...\n",
            "  Обработано: 12000 файлов...\n",
            "\n",
            "  Всего обработано: 12499 файлов\n",
            "  Ошибок: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nАНАЛИЗ ФОРМАТОВ ИЗОБРАЖЕНИЙ:\")\n",
        "print(\"\\nФорматы кошачьих изображений:\")\n",
        "print(cat_channels)\n",
        "print(\"\\nФорматы собачьих изображений:\")\n",
        "print(dog_channels)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Анализ размеров изображений\n",
        "analyze_sizes(cat_sizes, \"кошек\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "analyze_sizes(dog_sizes, \"собак\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6iCVkshPzXt",
        "outputId": "9fe8c8d5-139b-4357-ba36-16b38b37a547"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "АНАЛИЗ ФОРМАТОВ ИЗОБРАЖЕНИЙ:\n",
            "\n",
            "Форматы кошачьих изображений:\n",
            "{3: 12470, 1: 29}\n",
            "\n",
            "Форматы собачьих изображений:\n",
            "{3: 12461, 4: 5, 1: 33}\n",
            "\n",
            "==================================================\n",
            "\n",
            "АНАЛИЗ РАЗМЕРОВ ИЗОБРАЖЕНИЙ КОШЕК:\n",
            "  Ширина: от 4 до 500 (средняя: 411)\n",
            "  Высота: от 4 до 500 (средняя: 357)\n",
            "  Соотношение сторон: от 0.35 до 3.72\n",
            "\n",
            "    Классификация по размерам:\n",
            "    Очень маленькие (<32px): 1\n",
            "    Маленькие (32-64px): 16\n",
            "    Средние (64-128px): 258\n",
            "    Большие (128-256px): 2325\n",
            "    Очень большие (256-512px): 11353\n",
            "    Огромные (>512px): 0\n",
            "\n",
            "    Классификация по соотношениям сторон:\n",
            "    Очень узкие (<0.5): 34\n",
            "    Узкие (0.5-0.8): 1600\n",
            "    Квадратные (0.8-1.2): 3923\n",
            "    Широкие (1.2-2.0): 7209\n",
            "    Очень широкие (>2.0): 109\n",
            "\n",
            "==================================================\n",
            "\n",
            "АНАЛИЗ РАЗМЕРОВ ИЗОБРАЖЕНИЙ СОБАК:\n",
            "  Ширина: от 42 до 500 (средняя: 398)\n",
            "  Высота: от 33 до 500 (средняя: 365)\n",
            "  Соотношение сторон: от 0.31 до 5.94\n",
            "\n",
            "    Классификация по размерам:\n",
            "    Очень маленькие (<32px): 0\n",
            "    Маленькие (32-64px): 26\n",
            "    Средние (64-128px): 242\n",
            "    Большие (128-256px): 2240\n",
            "    Очень большие (256-512px): 11359\n",
            "    Огромные (>512px): 0\n",
            "\n",
            "    Классификация по соотношениям сторон:\n",
            "    Очень узкие (<0.5): 42\n",
            "    Узкие (0.5-0.8): 2242\n",
            "    Квадратные (0.8-1.2): 4411\n",
            "    Широкие (1.2-2.0): 6142\n",
            "    Очень широкие (>2.0): 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Выводы по качеству данных\n",
        "\n",
        "* У классов практически идентичное распределение\n",
        "* Большинство изображений больших размеров (256-512px) ~ 90%\n",
        "* Отличный баланс классов (50% кошек, 50% собак)\n",
        "* У большинства изображений 3 канала (RGB) ~ 99%\n",
        "* Изображений критически маленького размера или с экстремальными соотношениями сторон < 1%"
      ],
      "metadata": {
        "id": "ACMgRriDhFdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### План предобработки\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3fFfJqh3P_6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Удаление проблемных изображений\n",
        "1. Критически маленькие изображения\n",
        "\n",
        "- **Критерий**: ширина < 32px ИЛИ высота < 32px\n",
        "\n",
        "- **Причина**: невозможно распознать объект, только шум\n",
        "\n",
        "2. Изображения с экстремальными соотношениями сторон\n",
        "\n",
        "- **Критерий**: соотношение сторон < 0.4 ИЛИ соотношение сторон > 3.0\n",
        "- **Причина**: сильные искажения при resize, потеря информации\n"
      ],
      "metadata": {
        "id": "O7hVg9jUQ37u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Унификация каналов\n",
        "В нашем датасете есть следующие проблемы с разными каналами:\n",
        "1. **Grayscale** изображения (1 канал)\n",
        "- Проблема: модель ожидает 3 канала (RGB)\n",
        "- Решение: конвертация в RGB путем дублирования канала\n",
        "2. **RGBA** изображения (4 канала):\n",
        "- Проблема: лишний альфа-канал (прозрачность)\n",
        "- Решение: удаление альфа-канала, сохранение только RGB"
      ],
      "metadata": {
        "id": "JwhAhMPjRl0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resize и padding изображений\n",
        "Выбранный размер изображения после resize: **128×128** пикселей. Это баланс качества и производительности\n",
        "\n",
        "Для приведения всех изображений к единому размеру 128×128 пикселей будем применять двухэтапный подход:\n",
        "\n",
        "1. **Resize** с сохранением пропорций\n",
        "Будет использоваться метод thumbnail() библиотеки PIL\n",
        "- Цель: уменьшить изображение с сохранением оригинальных пропорций\n",
        "- Преимущество: избегаем искажений объектов\n",
        "- Пример: изображение 400×200 пропорционально уменьшается до 128×64\n",
        "2. **Padding** для создания квадрата\n",
        "После пропорционального уменьшения изображение помещается в центр черного квадрата\n",
        "\n",
        "\n",
        "**Этот подход гарантирует:**\n",
        "\n",
        "- Все изображения имеют размер 128×128 пикселей\n",
        "- Сохраняются оригинальные пропорции объектов\n",
        "- Отсутствуют искажения изображений\n",
        "- Черные поля не мешают классификации, так как содержат только фон\n"
      ],
      "metadata": {
        "id": "3rtbhTC4Rsrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка и подготовка данных"
      ],
      "metadata": {
        "id": "YoJPXOUkhnJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_dataset(folder, label, target_size, min_size):\n",
        "    \"\"\"\n",
        "    Полная загрузка и предобработка изображений из папки\n",
        "\n",
        "    Args:\n",
        "        folder: путь к папке с изображениями\n",
        "        target_size: целевой размер изображений (ширина, высота)\n",
        "        min_size: минимальный размер изображения\n",
        "\n",
        "    Returns:\n",
        "        X: массив изображений формы (n_samples, height, width, 3)\n",
        "        y: массив меток формы (n_samples)\n",
        "    \"\"\"\n",
        "\n",
        "    def normalize_channels(img_array):\n",
        "        \"\"\"\n",
        "        Приводит изображение к единому формату RGB (3 канала)\n",
        "        \"\"\"\n",
        "        if len(img_array.shape) == 2:\n",
        "            # Grayscale (1 канал) → RGB (3 канала)\n",
        "            img_array = np.stack([img_array] * 3, axis=-1)\n",
        "\n",
        "        elif len(img_array.shape) == 3:\n",
        "            if img_array.shape[2] == 1:\n",
        "                # (H, W, 1) → (H, W, 3)\n",
        "                img_array = np.concatenate([img_array] * 3, axis=-1)\n",
        "\n",
        "            elif img_array.shape[2] == 4:\n",
        "                # RGBA (4 канала) → RGB (3 канала)\n",
        "                img_array = img_array[:, :, :3]\n",
        "\n",
        "            elif img_array.shape[2] != 3:\n",
        "                # Неподдерживаемый формат\n",
        "                raise ValueError(f\"Неподдерживаемое количество каналов: {img_array.shape[2]}\")\n",
        "\n",
        "        return img_array\n",
        "\n",
        "    # def smart_resize_with_padding(img, target_size):\n",
        "    #     \"\"\"\n",
        "    #     Resize изображения с сохранением пропорций через padding\n",
        "    #     \"\"\"\n",
        "    #     # Уменьшаем с сохранением пропорций (thumbnail сохраняет пропорции)\n",
        "    #     img_copy = img.copy()\n",
        "    #     img_copy.thumbnail(target_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    #     # Создаем квадратный холст\n",
        "    #     new_img = Image.new('RGB', target_size, (0, 0, 0))  # Черный фон\n",
        "\n",
        "    #     # Центрируем изображение\n",
        "    #     x = (target_size[0] - img_copy.size[0]) // 2\n",
        "    #     y = (target_size[1] - img_copy.size[1]) // 2\n",
        "    #     new_img.paste(img_copy, (x, y))\n",
        "\n",
        "    #     return new_img\n",
        "\n",
        "    def smart_resize_with_padding(img, target_size):\n",
        "      \"\"\"\n",
        "      Resize изображения с сохранением пропорций через padding\n",
        "      \"\"\"\n",
        "      try:\n",
        "          # Уменьшаем с сохранением пропорций\n",
        "          img_copy = img.copy()\n",
        "          img_copy.thumbnail(target_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "          # Создаем квадратный холст нужного размера\n",
        "          new_img = Image.new('RGB', target_size, (0, 0, 0))  # Черный фон\n",
        "\n",
        "          # Центрируем изображение\n",
        "          x = (target_size[0] - img_copy.size[0]) // 2\n",
        "          y = (target_size[1] - img_copy.size[1]) // 2\n",
        "          new_img.paste(img_copy, (x, y))\n",
        "\n",
        "          # Проверяем финальный размер\n",
        "          if new_img.size != target_size:\n",
        "              # Принудительно изменяем до нужного размера если что-то пошло не так\n",
        "              new_img = new_img.resize(target_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "          return new_img\n",
        "      except Exception as e:\n",
        "          print(f\"Ошибка при resize: {e}\")\n",
        "          # Возвращаем стандартное изображение нужного размера\n",
        "          return Image.new('RGB', target_size, (128, 128, 128))  # Серый фон\n",
        "\n",
        "    def should_keep_image(width, height, aspect_ratio, min_size=32):\n",
        "        \"\"\"\n",
        "        Определяет, стоит ли оставлять изображение\n",
        "        \"\"\"\n",
        "        # Минимальный размер\n",
        "        if min(width, height) < min_size:\n",
        "            return False\n",
        "\n",
        "        # Экстремальные соотношения сторон\n",
        "        if aspect_ratio < 0.4 or aspect_ratio > 3.0:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "    stats = {\n",
        "        'processed': 0,\n",
        "        'deleted': 0,\n",
        "        'corrupted': 0\n",
        "    }\n",
        "\n",
        "    print(f\"Загружаем изображения из папки {os.path.basename(folder)}\")\n",
        "\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "            try:\n",
        "                img_path = os.path.join(folder, filename)\n",
        "\n",
        "                # Открываем изображение\n",
        "                img = Image.open(img_path)\n",
        "                width, height = img.size\n",
        "                aspect_ratio = width / height\n",
        "\n",
        "                # Проверяем, стоит ли оставлять изображение\n",
        "                if not should_keep_image(width, height, aspect_ratio, min_size):\n",
        "                    stats['deleted'] += 1\n",
        "                    continue\n",
        "\n",
        "                # Применяем smart resize с padding\n",
        "                img_resized = smart_resize_with_padding(img, target_size)\n",
        "                # img_resized = img.resize(target_size)\n",
        "\n",
        "                # Конвертируем в массив и нормализуем\n",
        "                img_array = np.array(img_resized) / 255.0\n",
        "\n",
        "                # Унифицируем каналы\n",
        "                if len(img_array.shape) == 2 or (len(img_array.shape) == 3 and img_array.shape[2] != 3):\n",
        "                  img_array = normalize_channels(img_array)\n",
        "\n",
        "                # Проверяем финальный формат\n",
        "                if img_array.shape == (*target_size, 3):\n",
        "                    images.append(img_array)\n",
        "                    labels.append(label)\n",
        "                    stats['processed'] += 1\n",
        "                else:\n",
        "                    stats['corrupted'] += 1\n",
        "\n",
        "                # Показываем прогресс\n",
        "                if stats['processed'] % 1000 == 0:\n",
        "                    print(f\"  Обработано: {stats['processed']} изображений...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                stats['corrupted'] += 1\n",
        "                if stats['corrupted'] <= 10:\n",
        "                    print(f\"  Ошибка при обработке {filename}: {type(e).__name__}\")\n",
        "\n",
        "    print(f\"\\n  Загрузка файлов завершена:\")\n",
        "    print(f\"  Загружено: {stats['processed']} изображений\")\n",
        "    print(f\"  Отфильтровано: {stats['deleted'] + stats['corrupted']} изображений\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return images, labels, stats"
      ],
      "metadata": {
        "id": "bhfe7wPwxBBd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возникают проблемы с изображениями 128x128"
      ],
      "metadata": {
        "id": "F22K1rQiNE0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Загружаем изображения обоих классов\n",
        "    min_size = 32\n",
        "    # target_size=(128, 128)\n",
        "    target_size=(64, 64)\n",
        "\n",
        "    cat_images, cat_labels, cat_stats = load_and_preprocess_dataset(\n",
        "        cat_folder, 0, target_size, min_size\n",
        "    )\n",
        "\n",
        "    # dog_images, dog_labels, dog_stats = load_and_preprocess_dataset(\n",
        "    #     dog_folder, 1, target_size, min_size\n",
        "    # )\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJjJZgQj0WqF",
        "outputId": "aa902ef2-7317-4363-e847-535c470419d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружаем изображения из папки Cat\n",
            "  Обработано: 1000 изображений...\n",
            "  Обработано: 2000 изображений...\n",
            "  Обработано: 3000 изображений...\n",
            "  Обработано: 4000 изображений...\n",
            "  Обработано: 5000 изображений...\n",
            "  Обработано: 6000 изображений...\n",
            "  Обработано: 7000 изображений...\n",
            "  Обработано: 8000 изображений...\n",
            "  Обработано: 9000 изображений...\n",
            "  Обработано: 10000 изображений...\n",
            "  Обработано: 11000 изображений...\n",
            "  Обработано: 12000 изображений...\n",
            "\n",
            "  Загрузка файлов завершена:\n",
            "  Загружено: 12489 изображений\n",
            "  Отфильтровано: 10 изображений\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # # Объединяем данные\n",
        "    # X = np.array(cat_images + dog_images)\n",
        "    # y = np.array(cat_labels + dog_labels)\n",
        "\n",
        "    X = np.array(cat_images)\n",
        "\n",
        "\n",
        "    # # Выводим статистику\n",
        "    # print(\"\\n\" + \"=\"*60)\n",
        "    # print(\"СТАТИСТИКА ЗАГРУЗКИ\")\n",
        "    # print(\"=\"*60)\n",
        "    # print(f\"Кошки: {len(cat_images)} изображений\")\n",
        "    # # print(f\"Собаки: {len(dog_images)} изображений\")\n",
        "    # # print(f\"Всего: {len(X)} изображений\")\n",
        "    # print(f\"Форма данных: {X.shape}\")\n",
        "    # print(f\"Диапазон значений: {X.min():.3f} - {X.max():.3f}\")\n",
        "\n",
        "    # print(f\"\\nСтатистика кошек:\")\n",
        "    # for key, value in cat_stats.items():\n",
        "    #     print(f\"  {key}: {value}\")\n",
        "\n",
        "    # print(f\"\\nСтатистика собак:\")\n",
        "    # for key, value in dog_stats.items():\n",
        "    #     print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "id": "CX1J20Cj08AH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def visualize_sample_images(X, y, n_samples=6):\n",
        "#     \"\"\"\n",
        "#     Визуализация примеров изображений\n",
        "#     \"\"\"\n",
        "#     plt.figure(figsize=(15, 6))\n",
        "\n",
        "#     # Выбираем случайные изображения\n",
        "#     indices = np.random.choice(len(X), min(n_samples, len(X)), replace=False)\n",
        "\n",
        "#     for i, idx in enumerate(indices):\n",
        "#         plt.subplot(2, 3, i + 1)\n",
        "#         plt.imshow(X[idx])\n",
        "#         plt.title(f\"{'Кошка' if y[idx] == 0 else 'Собака'}\")\n",
        "#         plt.axis('off')\n",
        "\n",
        "#     plt.suptitle('Примеры изображений после предобработки')\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# def split_dataset(X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
        "#     \"\"\"\n",
        "#     Разделение датасета на train/validation/test\n",
        "#     \"\"\"\n",
        "#     # Сначала делим на train+val и test\n",
        "#     X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "#         X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "#     )\n",
        "\n",
        "#     # Затем делим train+val на train и validation\n",
        "#     val_size_adjusted = val_size / (1 - test_size)  # Корректируем размер валидации\n",
        "#     X_train, X_val, y_train, y_val = train_test_split(\n",
        "#         X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state, stratify=y_temp\n",
        "#     )\n",
        "\n",
        "#     print(f\"Разделение датасета:\")\n",
        "#     print(f\"  Обучение: {len(X_train)} изображений ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "#     print(f\"  Валидация: {len(X_val)} изображений ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "#     print(f\"  Тест: {len(X_test)} изображений ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "#     # Проверяем баланс классов\n",
        "#     print(f\"\\nБаланс классов:\")\n",
        "#     print(f\"  Обучение - Кошки: {np.sum(y_train == 0)}, Собаки: {np.sum(y_train == 1)}\")\n",
        "#     print(f\"  Валидация - Кошки: {np.sum(y_val == 0)}, Собаки: {np.sum(y_val == 1)}\")\n",
        "#     print(f\"  Тест - Кошки: {np.sum(y_test == 0)}, Собаки: {np.sum(y_test == 1)}\")\n",
        "\n",
        "#     return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# # Использование:\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Пути к папкам (измени на свои)\n",
        "#     cat_folder = '/kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat'\n",
        "#     dog_folder = '/kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog'\n",
        "\n",
        "#     # Загружаем и предобрабатываем данные\n",
        "#     print(\"Начинаем загрузку и предобработку данных...\")\n",
        "#     X, y = load_and_preprocess_dataset(\n",
        "#         cat_folder=cat_folder,\n",
        "#         dog_folder=dog_folder,\n",
        "#         target_size=(128, 128),\n",
        "#         min_size=32\n",
        "#     )\n",
        "\n",
        "#     # Визуализируем примеры\n",
        "#     print(\"\\nПоказываем примеры изображений...\")\n",
        "#     visualize_sample_images(X, y)\n",
        "\n",
        "#     # Разделяем на выборки\n",
        "#     print(\"\\nРазделяем датасет...\")\n",
        "#     X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X, y)\n",
        "\n",
        "#     print(f\"\\n✅ Готово! Данные загружены и готовы для обучения.\")\n",
        "#     print(f\"   Форма обучающей выборки: {X_train.shape}\")\n",
        "#     print(f\"   Форма валидационной выборки: {X_val.shape}\")\n",
        "#     print(f\"   Форма тестовой выборки: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "4n1ehfUyzW2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbmZ-IvAOE45",
        "outputId": "429de933-c7d6-46b4-dd3a-b87cedc7cade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        }
      ],
      "source": [
        "def load_images(folder, label, img_size=(128, 128)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "      if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "        try:\n",
        "            img = Image.open(os.path.join(folder, filename))\n",
        "            img = img.resize(img_size)\n",
        "            img_array = np.array(img) / 255.0  # нормализация\n",
        "\n",
        "            # Grayscale изображения\n",
        "            if len(img_array.shape) == 2:\n",
        "                img_array = np.stack([img_array]*3, axis=-1)\n",
        "            elif img_array.shape[2] == 4:\n",
        "                # RGBA -> RGB: убираем альфа-канал\n",
        "                 img_array = img_array[:,:,:3]\n",
        "\n",
        "            images.append(img_array)\n",
        "            labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка: {filename} - {e}\")\n",
        "    return images, labels\n",
        "\n",
        "cat_images, cat_labels = load_images('/kaggle/input/dog-and-cat-classification-dataset/PetImages/Cat', 0)\n",
        "dog_images, dog_labels = load_images('/kaggle/input/dog-and-cat-classification-dataset/PetImages/Dog', 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(cat_images + dog_images)\n",
        "y = np.array(cat_labels + dog_labels)"
      ],
      "metadata": {
        "id": "CGVJUD8RPbKB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "0AN2aobTPfrN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение моделей\n",
        "1. Базовая модель\n",
        "2. Модель с регуляризацией\n",
        "3. Модель с аугментацией\n",
        "4. Финальная"
      ],
      "metadata": {
        "id": "JGbwCsm4idft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Модель 1: Базовая архитектура"
      ],
      "metadata": {
        "id": "hUO3Z-G3ihPT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJMH3kl9i3vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Создаем правильную архитектуру\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(64,64,3)),\n",
        "\n",
        "    layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Компилируем\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Теперь следим за loss\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Обучаем\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "FnLSxve3cKWb",
        "outputId": "7c2d3744-57a3-4d99-b1c8-13ea448dd098"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 208ms/step - accuracy: 0.5184 - loss: 0.6917 - val_accuracy: 0.6670 - val_loss: 0.6377\n",
            "Epoch 2/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 204ms/step - accuracy: 0.6728 - loss: 0.6139 - val_accuracy: 0.7374 - val_loss: 0.5286\n",
            "Epoch 3/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 203ms/step - accuracy: 0.7252 - loss: 0.5490 - val_accuracy: 0.7536 - val_loss: 0.4943\n",
            "Epoch 4/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 209ms/step - accuracy: 0.7606 - loss: 0.4981 - val_accuracy: 0.7846 - val_loss: 0.4578\n",
            "Epoch 5/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 205ms/step - accuracy: 0.7902 - loss: 0.4585 - val_accuracy: 0.8042 - val_loss: 0.4254\n",
            "Epoch 6/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 205ms/step - accuracy: 0.8003 - loss: 0.4356 - val_accuracy: 0.8166 - val_loss: 0.4039\n",
            "Epoch 7/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 204ms/step - accuracy: 0.8176 - loss: 0.4038 - val_accuracy: 0.8156 - val_loss: 0.3957\n",
            "Epoch 8/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 208ms/step - accuracy: 0.8243 - loss: 0.3905 - val_accuracy: 0.8226 - val_loss: 0.3848\n",
            "Epoch 9/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 214ms/step - accuracy: 0.8285 - loss: 0.3874 - val_accuracy: 0.8400 - val_loss: 0.3521\n",
            "Epoch 10/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 206ms/step - accuracy: 0.8323 - loss: 0.3805 - val_accuracy: 0.8396 - val_loss: 0.3569\n",
            "Epoch 11/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 220ms/step - accuracy: 0.8396 - loss: 0.3664 - val_accuracy: 0.8558 - val_loss: 0.3334\n",
            "Epoch 12/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 206ms/step - accuracy: 0.8470 - loss: 0.3519 - val_accuracy: 0.8468 - val_loss: 0.3368\n",
            "Epoch 13/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 210ms/step - accuracy: 0.8466 - loss: 0.3494 - val_accuracy: 0.8480 - val_loss: 0.3390\n",
            "Epoch 14/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 231ms/step - accuracy: 0.8520 - loss: 0.3390 - val_accuracy: 0.8602 - val_loss: 0.3229\n",
            "Epoch 15/30\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 214ms/step - accuracy: 0.8558 - loss: 0.3301 - val_accuracy: 0.8410 - val_loss: 0.3552\n",
            "Epoch 16/30\n",
            "\u001b[1m618/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8645 - loss: 0.3205"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1316619823.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Обучаем\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9WauD-dcyOG",
        "outputId": "3d9710f1-a28d-45b6-bae8-661324c14168"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.8434 - loss: 0.3617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p8kKTSDGczsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}